import cv2
import numpy as np
from MediaPipeAnalyzer import MediaPipeAnalyzer
from face_analyzer import FaceAnalyzer
from pose_analyzer import PoseAnalyzer
from hand_analyzer import HandAnalyzer
from movement_analyzer import MovementAnalyzer
from frame_analyzer import FrameAnalyzer

class VideoAnalyzer:
    """
    Analyseur vidÃ©o principal qui coordonne tous les analyzers spÃ©cialisÃ©s
    pour une analyse complÃ¨te des mouvements et expressions.
    """
    
    def __init__(self):
        """Initialise tous les analyzers nÃ©cessaires"""
        # Initialiser MediaPipe
        self.mediapipe_analyzer = MediaPipeAnalyzer()
        
        # Initialiser les analyzers spÃ©cialisÃ©s
        self.face_analyzer = FaceAnalyzer()
        self.pose_analyzer = PoseAnalyzer()
        self.hand_analyzer = HandAnalyzer()
        self.movement_analyzer = MovementAnalyzer()
        
        # Initialiser le FrameAnalyzer avec tous les composants nÃ©cessaires
        self.frame_analyzer = FrameAnalyzer(
            self.mediapipe_analyzer.face_mesh,
            self.mediapipe_analyzer.pose,
            self.mediapipe_analyzer.hands,
            self.face_analyzer,
            self.pose_analyzer,
            self.hand_analyzer
        )
        
        print("âœ… VideoAnalyzer initialisÃ© avec tous les analyzers (y compris FrameAnalyzer)")
    
    def _analyze_frame(self, frame, frame_num, fps):
        """
        Analyse un frame individuel en utilisant le FrameAnalyzer.
        
        Args:
            frame: Image du frame Ã  analyser (format RGB)
            frame_num: NumÃ©ro du frame dans la vidÃ©o
            fps: Nombre d'images par seconde de la vidÃ©o
            
        Returns:
            dict: RÃ©sultats d'analyse pour ce frame
        """
        # DÃ©lÃ©guer l'analyse au FrameAnalyzer
        return self.frame_analyzer.analyze_frame(frame, frame_num, fps)
    
    def _calculate_movement_metrics(self, results):
        """DÃ©lÃ¨gue le calcul des mÃ©triques au MovementAnalyzer"""
        return self.movement_analyzer._calculate_movement_metrics(results)

    def analyze_video(self, video_path, skip_frames=20):
        """Analyse complÃ¨te des mouvements dans la vidÃ©o"""
        try:
            # Afficher le dÃ©but de l'analyse avec le nom du fichier vidÃ©o
            print(f"ğŸ”„ Analyse des mouvements vidÃ©o : {video_path}")
            
            # Ouvrir la vidÃ©o Ã  analyser avec OpenCV
            cap = cv2.VideoCapture(video_path)
            if not cap.isOpened():
                # Si la vidÃ©o ne s'ouvre pas, lever une erreur
                raise IOError("Impossible d'ouvrir la vidÃ©o")
            
            # RÃ©cupÃ©rer les propriÃ©tÃ©s principales de la vidÃ©o : fps et nombre de frames
            fps = cap.get(cv2.CAP_PROP_FPS)
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            
            # Initialiser la structure de rÃ©sultats pour stocker les donnÃ©es analysÃ©es
            results = {
                'face_analysis': [],        # RÃ©sultats d'analyse du visage
                'pose_analysis': [],        # RÃ©sultats d'analyse de la posture
                'hand_analysis': [],        # RÃ©sultats d'analyse des mains
                'eye_contact': [],          # RÃ©sultats du contact visuel
                'facial_expressions': [],   # RÃ©sultats des expressions faciales
                'metadata': {               # MÃ©tadonnÃ©es sur la vidÃ©o
                    'fps': fps,
                    'total_frames': total_frames,
                    'duration': total_frames / fps if fps > 0 else 0
                }
            }
            
            frame_count = 0         # Compteur de frames lues
            processed_frames = 0    # Compteur de frames effectivement analysÃ©es
            
            # Boucle principale : lire les frames une par une
            while cap.isOpened():
                ret, frame = cap.read()    # Lire la prochaine frame
                if not ret:
                    # Si aucune frame n'est lue (fin de vidÃ©o), sortir de la boucle
                    break
                
                # Pour accÃ©lÃ©rer l'analyse, ne traiter qu'une frame tous les skip_frames
                if frame_count % skip_frames == 0:
                    # Convertir le format d'image de BGR (OpenCV) Ã  RGB (MediaPipe)
                    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                    
                    # Appeler la mÃ©thode d'analyse sur la frame courante
                    frame_results = self._analyze_frame(rgb_frame, frame_count, fps)
                    
                    # Stocker les rÃ©sultats de chaque type d'analyse si prÃ©sents
                    if frame_results['face']:
                        results['face_analysis'].append(frame_results['face'])
                    if frame_results['pose']:
                        results['pose_analysis'].append(frame_results['pose'])
                    if frame_results['hands']:
                        results['hand_analysis'].append(frame_results['hands'])
                    if frame_results['eye_contact'] is not None:
                        results['eye_contact'].append(frame_results['eye_contact'])
                    if frame_results['expressions']:
                        results['facial_expressions'].append(frame_results['expressions'])
                    
                    processed_frames += 1  # IncrÃ©menter le compteur des frames traitÃ©es
                    
                    # Afficher la progression de l'analyse tous les 20 frames traitÃ©es
                    if processed_frames % 20 == 0:
                        progress = (frame_count / total_frames) * 100
                        print(f"   ğŸ“Š Progression: {progress:.1f}%")
                
                frame_count += 1    # IncrÃ©menter le compteur de frames lues
            
            # LibÃ©rer la ressource vidÃ©o Ã  la fin de l'analyse
            cap.release()
            
            # Calculer les mÃ©triques globales des mouvements Ã  partir des rÃ©sultats
            movement_metrics = self._calculate_movement_metrics(results)
            results['movement_metrics'] = movement_metrics
            
            # Afficher un message de fin et retourner les rÃ©sultats
            print(f"âœ… Analyse vidÃ©o terminÃ©e ({processed_frames} frames analysÃ©s)")
            return results
            
        except Exception as e:
            # En cas d'erreur, afficher le message et retourner None
            print(f"âŒ Erreur d'analyse vidÃ©o : {str(e)}")
            return None


# Fonction utilitaire pour utiliser facilement le VideoAnalyzer
def analyze_video_file(video_path, skip_frames=20, detailed_output=True):
    """
    Fonction utilitaire pour analyser une vidÃ©o facilement.
    
    Args:
        video_path (str): Chemin vers le fichier vidÃ©o
        skip_frames (int): Nombre de frames Ã  ignorer entre chaque analyse
        detailed_output (bool): Afficher les rÃ©sultats dÃ©taillÃ©s
        
    Returns:
        dict: RÃ©sultats complets de l'analyse ou None en cas d'erreur
    """
    analyzer = VideoAnalyzer()
    results = analyzer.analyze_video(video_path, skip_frames)
    
    if results and detailed_output:
        _afficher_resultats_detailles(results, video_path)
    
    return results

def _afficher_resultats_detailles(results, video_path):
    """Affiche les rÃ©sultats de l'analyse de maniÃ¨re dÃ©taillÃ©e"""
    
    print("\n" + "="*60)
    print("ğŸ¬ ANALYSE VIDÃ‰O DÃ‰TAILLÃ‰E")
    print("="*60)
    print(f"ğŸ“‚ Fichier: {video_path}")
    
    # === MÃ‰TADONNÃ‰ES ===
    print("\n" + "="*60)
    print("ğŸ“¹ MÃ‰TADONNÃ‰ES DE LA VIDÃ‰O")
    print("="*60)
    metadata = results['metadata']
    print(f"â±ï¸  DurÃ©e: {metadata['duration']:.1f} secondes")
    print(f"ğŸï¸  FPS: {metadata['fps']:.1f}")
    print(f"ğŸ“Š Total frames: {metadata['total_frames']}")
    
    # === DÃ‰TECTIONS ===
    print("\n" + "="*60)
    print("ğŸ” DÃ‰TECTIONS PAR CATÃ‰GORIE")
    print("="*60)
    print(f"ğŸ‘¤ Frames avec visage analysÃ©: {len(results['face_analysis'])}")
    print(f"ğŸƒ Frames avec posture analysÃ©e: {len(results['pose_analysis'])}")
    print(f"âœ‹ Frames avec mains analysÃ©es: {len(results['hand_analysis'])}")
    print(f"ğŸ‘ï¸  Frames avec contact visuel: {len(results['eye_contact'])}")
    print(f"ğŸ˜Š Frames avec expressions: {len(results['facial_expressions'])}")
    
    # === MÃ‰TRIQUES GLOBALES ===
    if 'movement_metrics' in results:
        print("\n" + "="*60)
        print("ğŸ“Š MÃ‰TRIQUES COMPORTEMENTALES GLOBALES")
        print("="*60)
        metrics = results['movement_metrics']
        
        if 'eye_contact_percentage' in metrics:
            print(f"ğŸ‘ï¸  Contact visuel: {metrics['eye_contact_percentage']:.1f}%")
        
        if 'smile_percentage' in metrics:
            print(f"ğŸ˜Š Pourcentage de sourires: {metrics['smile_percentage']:.1f}%")
        
        if 'hands_visible_percentage' in metrics:
            print(f"âœ‹ Mains visibles: {metrics['hands_visible_percentage']:.1f}%")
        
        if 'head_movement' in metrics:
            head = metrics['head_movement']
            print(f"ğŸ”„ VariabilitÃ© inclinaison tÃªte: {head.get('tilt_variance', 0):.2f}")
            print(f"ğŸ“ Inclinaison moyenne: {head.get('average_tilt', 0):.2f}Â°")
        
        if 'posture_stability' in metrics:
            posture = metrics['posture_stability']
            print(f"ğŸƒ StabilitÃ© Ã©paules: {posture.get('shoulder_stability', 0):.1f}/100")
            print(f"ğŸ§ StabilitÃ© corps: {posture.get('body_stability', 0):.1f}/100")
    
    # === Ã‰CHANTILLON DE DONNÃ‰ES ===
    print("\n" + "="*60)
    print("ğŸ“‹ Ã‰CHANTILLON DE DONNÃ‰ES FRAME PAR FRAME")
    print("="*60)
    
    # Montrer les premiers rÃ©sultats dÃ©taillÃ©s
    if results['face_analysis']:
        print("\nğŸ”¸ ANALYSE DU VISAGE (5 premiers frames):")
        for i, face in enumerate(results['face_analysis'][:5]):
            print(f"  ğŸ“ Frame {face['frame']} ({face['timestamp']:.1f}s):")
            print(f"     â€¢ Inclinaison tÃªte: {face.get('head_tilt', 'N/A'):.1f}Â°")
            print(f"     â€¢ Hochement: {face.get('head_nod', 'N/A'):.1f}Â°")
            print(f"     â€¢ Rotation: {face.get('head_turn', 'N/A'):.1f}Â°")
            if face.get('is_smiling'):
                print(f"     â€¢ ğŸ˜Š SOURIT (intensitÃ©: {face.get('expression_intensity', 0):.2f})")
    
    if results['eye_contact']:
        print("\nğŸ”¸ CONTACT VISUEL (5 premiers frames):")
        for i, eye in enumerate(results['eye_contact'][:5]):
            status = "ğŸ‘ï¸  REGARDE" if eye.get('looking_at_camera') else "ğŸ‘ï¸  DÃ‰TOURNÃ‰"
            print(f"  ğŸ“ Frame {eye['frame']}: {status}")
            print(f"     â€¢ Direction X: {eye.get('eye_direction_x', 0):.3f}")
            print(f"     â€¢ Direction Y: {eye.get('eye_direction_y', 0):.3f}")
    
    if results['pose_analysis']:
        print("\nğŸ”¸ ANALYSE POSTURALE (5 premiers frames):")
        for i, pose in enumerate(results['pose_analysis'][:5]):
            print(f"  ğŸ“ Frame {pose['frame']}:")
            print(f"     â€¢ Inclinaison Ã©paules: {pose.get('shoulder_tilt', 0):.1f}Â°")
            print(f"     â€¢ Largeur Ã©paules: {pose.get('shoulder_width', 0):.3f}")
    
    if results['hand_analysis']:
        print("\nğŸ”¸ ANALYSE DES MAINS (5 premiers frames):")
        for i, hands in enumerate(results['hand_analysis'][:5]):
            print(f"  ğŸ“ Frame {hands['frame']}:")
            print(f"     â€¢ Mains dÃ©tectÃ©es: {hands.get('hands_detected', 0)}")
            if 'hands_data' in hands and hands['hands_data']:
                for j, hand in enumerate(hands['hands_data']):
                    print(f"     â€¢ Main {hand.get('hand', 'Unknown')}: {hand.get('fingers_extended', 0)} doigts - {hand.get('gesture_type', 'Unknown')}")
    
    # === SAUVEGARDE JSON ===
    _sauvegarder_resultats_json(results)

def _sauvegarder_resultats_json(results):
    """Sauvegarde les rÃ©sultats en JSON"""
    import json
    
    output_file = "resultats_analyse_complete.json"
    
    # Fonction pour convertir les types non-JSON en types compatibles
    def convert_for_json(obj):
        if isinstance(obj, (bool, int, float, str, type(None))):
            return obj
        elif isinstance(obj, dict):
            return {k: convert_for_json(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [convert_for_json(item) for item in obj]
        else:
            return str(obj)  # Convertir tout le reste en string
    
    try:
        # Convertir les rÃ©sultats pour JSON
        json_compatible_results = convert_for_json(results)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(json_compatible_results, f, indent=2, ensure_ascii=False)
        
        print("\n" + "="*60)
        print("ğŸ’¾ SAUVEGARDE JSON")
        print("="*60)
        print(f"ğŸ“ RÃ©sultats complets sauvegardÃ©s dans: {output_file}")
        
    except Exception as e:
        print("\n" + "="*60)
        print("âš ï¸  ERREUR DE SAUVEGARDE JSON")
        print("="*60)
        print(f"âŒ Impossible de sauvegarder: {e}")
        print("âœ… Mais l'analyse est complÃ¨te et les rÃ©sultats sont disponibles !")
    
    print("\n" + "="*60)
    print("âœ… ANALYSE TERMINÃ‰E AVEC SUCCÃˆS !")
    print("="*60)


# Exemple d'utilisation
if __name__ == "__main__":
    # Exemple d'utilisation du VideoAnalyzer
    video_path = "test.mp4"  # Remplacer par le chemin de votre vidÃ©o
    
    print("ğŸš€ DÃ©marrage de l'analyse vidÃ©o...")
    results = analyze_video_file(video_path, skip_frames=30)
    
    if results:
        print("\nğŸ“ˆ RÃ©sultats de l'analyse :")
        print(f"   DurÃ©e vidÃ©o: {results['metadata']['duration']:.1f}s")
        print(f"   Frames analysÃ©s: {len(results['face_analysis'])}")
        
        if 'movement_metrics' in results:
            metrics = results['movement_metrics']
            if 'eye_contact_percentage' in metrics:
                print(f"   Contact visuel: {metrics['eye_contact_percentage']:.1f}%")
            if 'smile_percentage' in metrics:
                print(f"   Sourires: {metrics['smile_percentage']:.1f}%")
    else:
        print("âŒ Ã‰chec de l'analyse vidÃ©o")