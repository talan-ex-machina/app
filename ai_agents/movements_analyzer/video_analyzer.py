import cv2
import numpy as np
from MediaPipeAnalyzer import MediaPipeAnalyzer
from face_analyzer import FaceAnalyzer
from pose_analyzer import PoseAnalyzer
from hand_analyzer import HandAnalyzer
from frame_analyzer import FrameAnalyzer

class VideoAnalyzer:
    """
    Analyseur vid√©o principal qui coordonne tous le    print(f"     print(" M√©triques temporairement d√©sactiv√©es")
    
    print("\n" + "="*60)
    print(" ANALYSE TERMIN√âE AVEC SUCC√àS !")
    print("="*60)

def _sauvegarder_resultats_json(results):avec contact visuel: {len(results['eye_contact'])}")
    print(f" Frames avec expressions: {len(results['facial_expressions'])}")
    
    # === SAUVEGARDE JSON ===p√©cialis√©s
    pour une analyse compl√®te des mouvements et expressions.
    """
    
    def __init__(self):
        """Initialise tous les analyzers n√©cessaires"""
        # Initialiser MediaPipe
        self.mediapipe_analyzer = MediaPipeAnalyzer()
        
        # Initialiser les analyzers sp√©cialis√©s
        self.face_analyzer = FaceAnalyzer()
        self.pose_analyzer = PoseAnalyzer()
        self.hand_analyzer = HandAnalyzer()
        
        # Initialiser le FrameAnalyzer avec tous les composants n√©cessaires
        self.frame_analyzer = FrameAnalyzer(
            self.mediapipe_analyzer.face_mesh,
            self.mediapipe_analyzer.pose,
            self.mediapipe_analyzer.hands,
            self.face_analyzer,
            self.pose_analyzer,
            self.hand_analyzer
        )
        
        print(" VideoAnalyzer initialis√© avec tous les analyzers (FrameAnalyzer inclus)")

    def _analyze_frame(self, frame, frame_num, fps):
        """
        Analyse un frame individuel en utilisant le FrameAnalyzer.
        
        Args:
            frame: Image du frame √† analyser (format RGB)
            frame_num: Num√©ro du frame dans la vid√©o
            fps: Nombre d'images par seconde de la vid√©o
            
        Returns:
            dict: R√©sultats d'analyse pour ce frame
        """
        # D√©l√©guer l'analyse au FrameAnalyzer
        return self.frame_analyzer.analyze_frame(frame, frame_num, fps)

    def _get_raw_landmarks(self, frame):
        """
        Extrait les landmarks bruts de MediaPipe pour l'affichage.
        
        Args:
            frame: Image du frame √† analyser (format RGB)
            
        Returns:
            dict: Landmarks bruts pour l'affichage
        """
        landmarks = {
            'face_landmarks': [],
            'hand_landmarks': [],
            'pose_landmarks': []
        }
        
        # Extraire landmarks du visage
        face_results = self.mediapipe_analyzer.face_mesh.process(frame)
        if face_results.multi_face_landmarks:
            h, w = frame.shape[:2]
            for face_landmarks in face_results.multi_face_landmarks:
                landmarks['face_landmarks'] = [
                    (int(lm.x * w), int(lm.y * h)) 
                    for lm in face_landmarks.landmark
                ]
        
        # Extraire landmarks des mains
        hands_results = self.mediapipe_analyzer.hands.process(frame)
        if hands_results.multi_hand_landmarks:
            h, w = frame.shape[:2]
            for hand_landmarks in hands_results.multi_hand_landmarks:
                hand_points = [
                    (int(lm.x * w), int(lm.y * h)) 
                    for lm in hand_landmarks.landmark
                ]
                landmarks['hand_landmarks'].append(hand_points)
        
        # Extraire landmarks de la pose
        pose_results = self.mediapipe_analyzer.pose.process(frame)
        if pose_results.pose_landmarks:
            h, w = frame.shape[:2]
            landmarks['pose_landmarks'] = [
                (int(lm.x * w), int(lm.y * h)) 
                for lm in pose_results.pose_landmarks.landmark
            ]
        
        return landmarks

    def _afficher_resultats_frame(self, frame_results, frame_num):
        """
        Affiche les r√©sultats d'analyse d'une frame en temps r√©el dans la console.
        
        Args:
            frame_results: R√©sultats d'analyse pour cette frame
            frame_num: Num√©ro de la frame
        """
        print(f"\n FRAME {frame_num} ({frame_results['timestamp']:.1f}s)")
        print("‚îÄ" * 40)
        
        # Afficher les r√©sultats du visage
        if frame_results['face']:
            face = frame_results['face']
            print(f" VISAGE:")
            print(f"  ‚Ä¢ Inclinaison: {face.get('head_tilt', 0):.1f}¬∞")
            print(f"  ‚Ä¢ Hochement: {face.get('head_nod', 0):.1f}¬∞")
            print(f"  ‚Ä¢ Rotation: {face.get('head_turn', 0):.1f}¬∞")
            if face.get('is_smiling'):
                print(f"   ‚Ä¢  SOURIT (intensit√©: {face.get('expression_intensity', 0):.2f})")
            else:
                print(f"   ‚Ä¢  Pas de sourire")
        else:
            print(" VISAGE:  Non d√©tect√©")
        
        # Afficher les r√©sultats du contact visuel
        if frame_results['eye_contact']:
            eye = frame_results['eye_contact']
            status = " REGARDE" if eye.get('looking_at_camera') else "üëÅÔ∏è D√âTOURN√â"
            print(f"  REGARD: {status}")
            print(f"   ‚Ä¢ Direction X: {eye.get('eye_direction_x', 0):.3f}")
            print(f"   ‚Ä¢ Direction Y: {eye.get('eye_direction_y', 0):.3f}")
        else:
            print(" REGARD:  Non analys√©")
        
        # Afficher les r√©sultats de la posture
        if frame_results['pose']:
            pose = frame_results['pose']
            print(f" POSTURE:")
            print(f"   ‚Ä¢ Inclinaison √©paules: {pose.get('shoulder_tilt', 0):.1f}¬∞")
            print(f"   ‚Ä¢ Largeur √©paules: {pose.get('shoulder_width', 0):.3f}")
        else:
            print(" POSTURE:  Non d√©tect√©e")
        
        # Afficher les r√©sultats des mains
        if frame_results['hands']:
            hands = frame_results['hands']
            print(f" MAINS: {hands.get('hands_detected', 0)} d√©tect√©e(s)")
            if 'hands_data' in hands and hands['hands_data']:
                for i, hand in enumerate(hands['hands_data']):
                    print(f"   ‚Ä¢ Main {hand.get('hand', 'Unknown')}: {hand.get('fingers_extended', 0)} doigts - {hand.get('gesture_type', 'Unknown')}")
        else:
            print(" MAINS:  Non d√©tect√©es")

    def _draw_landmarks(self, frame, landmarks):
        """
        Dessine les landmarks d√©tect√©s sur le frame vid√©o.
        Adapte ce code selon la structure de landmarks.
        """
        # Dessin landmarks visage
        if 'face_landmarks' in landmarks and landmarks['face_landmarks']:
            for pt in landmarks['face_landmarks']:
                x, y = int(pt[0]), int(pt[1])
                cv2.circle(frame, (x, y), 2, (0, 255, 0), -1)
        # Dessin landmarks mains
        if 'hand_landmarks' in landmarks and landmarks['hand_landmarks']:
            for hand in landmarks['hand_landmarks']:
                for pt in hand:
                    x, y = int(pt[0]), int(pt[1])
                    cv2.circle(frame, (x, y), 2, (255, 0, 0), -1)
        # Dessin landmarks pose (corps)
        if 'pose_landmarks' in landmarks and landmarks['pose_landmarks']:
            for pt in landmarks['pose_landmarks']:
                x, y = int(pt[0]), int(pt[1])
                cv2.circle(frame, (x, y), 2, (0, 0, 255), -1)
        return frame

    def analyze_video(self, video_path, skip_frames=20, show_landmarks=False, show_frame_details=False):
        """Analyse compl√®te des mouvements dans la vid√©o"""
        try:
            # Afficher le d√©but de l'analyse avec le nom du fichier vid√©o
            print(f" Analyse des mouvements vid√©o : {video_path}")
            
            # Ouvrir la vid√©o √† analyser avec OpenCV
            cap = cv2.VideoCapture(video_path)
            if not cap.isOpened():
                # Si la vid√©o ne s'ouvre pas, lever une erreur
                raise IOError("Impossible d'ouvrir la vid√©o")
            
            # R√©cup√©rer les propri√©t√©s principales de la vid√©o : fps et nombre de frames
            fps = cap.get(cv2.CAP_PROP_FPS)
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            
            # Initialiser la structure de r√©sultats pour stocker les donn√©es analys√©es
            results = {
                'face_analysis': [],        # R√©sultats d'analyse du visage
                'pose_analysis': [],        # R√©sultats d'analyse de la posture
                'hand_analysis': [],        # R√©sultats d'analyse des mains
                'eye_contact': [],          # R√©sultats du contact visuel
                'facial_expressions': [],   # R√©sultats des expressions faciales
                'metadata': {               # M√©tadonn√©es sur la vid√©o
                    'fps': fps,
                    'total_frames': total_frames,
                    'duration': total_frames / fps if fps > 0 else 0
                }
            }
            
            frame_count = 0         # Compteur de frames lues
            processed_frames = 0    # Compteur de frames effectivement analys√©es
            
            # Boucle principale : lire les frames une par une
            while cap.isOpened():
                ret, frame = cap.read()    # Lire la prochaine frame
                if not ret:
                    # Si aucune frame n'est lue (fin de vid√©o), sortir de la boucle
                    break
                
                # Pour acc√©l√©rer l'analyse, ne traiter qu'une frame tous les skip_frames
                # Si skip_frames=0, analyser toutes les frames
                if skip_frames == 0 or frame_count % skip_frames == 0:
                    # Convertir le format d'image de BGR (OpenCV) √† RGB (MediaPipe)
                    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                    
                    # Appeler la m√©thode d'analyse sur la frame courante
                    frame_results = self._analyze_frame(rgb_frame, frame_count, fps)
                    
                    # Afficher les r√©sultats de cette frame en temps r√©el si demand√©
                    if show_frame_details:
                        self._afficher_resultats_frame(frame_results, frame_count)
                    
                    # Stocker les r√©sultats de chaque type d'analyse si pr√©sents
                    if frame_results['face']:
                        results['face_analysis'].append(frame_results['face'])
                    if frame_results['pose']:
                        results['pose_analysis'].append(frame_results['pose'])
                    if frame_results['hands']:
                        results['hand_analysis'].append(frame_results['hands'])
                    if frame_results['eye_contact'] is not None:
                        results['eye_contact'].append(frame_results['eye_contact'])
                    if frame_results['expressions']:
                        results['facial_expressions'].append(frame_results['expressions'])
                    
                    processed_frames += 1  # Incr√©menter le compteur des frames trait√©es
                    
                    # Affichage vid√©o + landmarks si demand√©
                    if show_landmarks:
                        # Obtenir les landmarks bruts pour l'affichage
                        raw_landmarks = self._get_raw_landmarks(rgb_frame)
                        # Dessiner les landmarks sur le frame
                        frame_annotated = self._draw_landmarks(frame.copy(), raw_landmarks)
                        cv2.imshow("Analyse Vid√©o - Landmarks", frame_annotated)
                        # Quitter en appuyant sur 'q'
                        if cv2.waitKey(1) & 0xFF == ord('q'):
                            break
                    
                    # Afficher la progression de l'analyse tous les 20 frames trait√©es
                    if processed_frames % 20 == 0:
                        progress = (frame_count / total_frames) * 100
                        print(f"    Progression: {progress:.1f}%")
                
                frame_count += 1    # Incr√©menter le compteur de frames lues
            
            # Lib√©rer la ressource vid√©o √† la fin de l'analyse
            cap.release()
            if show_landmarks:
                cv2.destroyAllWindows()
            
            # Afficher un message de fin et retourner les r√©sultats
            print(f" Analyse vid√©o termin√©e ({processed_frames} frames analys√©s)")
            return results
            
        except Exception as e:
            # En cas d'erreur, afficher le message et retourner None
            print(f" Erreur d'analyse vid√©o : {str(e)}")
            if show_landmarks:
                cv2.destroyAllWindows()
            return None


# Fonction utilitaire pour utiliser facilement le VideoAnalyzer
def analyze_video_file(video_path, skip_frames=0, detailed_output=True, show_landmarks=False, show_frame_details=False, output_folder=None):
    """
    Fonction utilitaire pour analyser une vid√©o facilement.
    
    Args:
        video_path (str): Chemin vers le fichier vid√©o
        skip_frames (int): Nombre de frames √† ignorer entre chaque analyse
        detailed_output (bool): Afficher les r√©sultats d√©taill√©s
        show_landmarks (bool): Afficher la vid√©o avec landmarks en temps r√©el
        show_frame_details (bool): Afficher les d√©tails de chaque frame dans la console
        output_folder (str): Dossier de sortie pour les fichiers JSON (optionnel)
        
    Returns:
        dict: R√©sultats complets de l'analyse ou None en cas d'erreur
    """
    analyzer = VideoAnalyzer()
    results = analyzer.analyze_video(video_path, skip_frames, show_landmarks, show_frame_details)
    
    # Toujours sauvegarder les r√©sultats, m√™me en mode silencieux
    if results:
        _sauvegarder_resultats_json(results, output_folder)
    
    # Afficher le rapport d√©taill√© seulement si demand√©
    if results and detailed_output:
        _afficher_resultats_detailles(results, video_path)
    
    return results



def _afficher_resultats_detailles(results, video_path):
    """Affiche les r√©sultats de l'analyse de mani√®re d√©taill√©e"""
    
    print("\n" + "="*60)
    print(" ANALYSE VID√âO D√âTAILL√âE")
    print("="*60)
    print(f" Fichier: {video_path}")
    
    # === M√âTADONN√âES ===
    print("\n" + "="*60)
    print(" M√âTADONN√âES DE LA VID√âO")
    print("="*60)
    metadata = results['metadata']
    print(f" Dur√©e: {metadata['duration']:.1f} secondes")
    print(f"  FPS: {metadata['fps']:.1f}")
    print(f" Total frames: {metadata['total_frames']}")
    
    # === D√âTECTIONS ===
    print("\n" + "="*60)
    print(" D√âTECTIONS PAR CAT√âGORIE")
    print("="*60)
    print(f" Frames avec visage analys√©: {len(results['face_analysis'])}")
    print(f" Frames avec posture analys√©e: {len(results['pose_analysis'])}")
    print(f" Frames avec mains analys√©es: {len(results['hand_analysis'])}")
    print(f"  Frames avec contact visuel: {len(results['eye_contact'])}")
    print(f" Frames avec expressions: {len(results['facial_expressions'])}")
 

 
    # === SAUVEGARDE JSON ===
    _sauvegarder_resultats_json(results)

def _sauvegarder_resultats_json(results, output_folder=None):
    """Sauvegarde les r√©sultats en JSON avec horodatage"""
    import json
    import datetime
    import os
    
    # Cr√©er un nom de fichier avec horodatage
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"resultats_analyse_{timestamp}.json"
    
    # D√©terminer le chemin complet
    if output_folder:
        output_file = os.path.join(output_folder, filename)
    else:
        output_file = filename
    
    # Fonction pour convertir les types non-JSON en types compatibles
    def convert_for_json(obj):
        if isinstance(obj, (bool, int, float, str, type(None))):
            return obj
        elif isinstance(obj, dict):
            return {k: convert_for_json(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [convert_for_json(item) for item in obj]
        else:
            return str(obj)  # Convertir tout le reste en string
    
    try:
        # Convertir les r√©sultats pour JSON
        json_compatible_results = convert_for_json(results)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(json_compatible_results, f, indent=2, ensure_ascii=False)
        
        print("\n" + "="*60)
        print(" SAUVEGARDE JSON")
        print("="*60)
        print(f" R√©sultats complets sauvegard√©s dans: {output_file}")
        
    except Exception as e:
        print("\n" + "="*60)
        print("  ERREUR DE SAUVEGARDE JSON")
        print("="*60)
        print(f" Impossible de sauvegarder: {e}")
        print(" Mais l'analyse est compl√®te et les r√©sultats sont disponibles !")
    
    print("\n" + "="*60)
    print(" ANALYSE TERMIN√âE AVEC SUCC√àS !")
    print("="*60)


# Exemple d'utilisation
if __name__ == "__main__":
    # Interface utilisateur interactive
    print(" ANALYSEUR VID√âO INTERACTIF")
    print("="*50)
    
    # Demander le chemin de la vid√©o
    print("\n S√âLECTION DE LA VID√âO")
    video_path = input("Entrez le chemin de votre vid√©o (ou appuyez sur Entr√©e pour 'test.mp4'): ").strip()
    if not video_path:
        video_path = "test.mp4"
    
    # V√©rifier si le fichier existe
    import os
    if not os.path.exists(video_path):
        print(f" Erreur : Le fichier '{video_path}' n'existe pas.")
        print(" Fichiers vid√©o disponibles dans le dossier actuel :")
        for file in os.listdir("."):
            if file.endswith(('.mp4', '.avi', '.mov', '.mkv', '.wmv')):
                print(f"   - {file}")
        exit()
    
    # Demander le nombre de frames √† ignorer
    print("\n‚ö° CONFIGURATION DE L'ANALYSE")
    print(" skip_frames = 0 : analyser toutes les frames (plus lent mais plus pr√©cis)")
    print(" skip_frames = 10 : analyser 1 frame sur 10 (plus rapide)")
    print(" skip_frames = 30 : analyser 1 frame sur 30 (tr√®s rapide)")
    
    skip_input = input("Entrez le nombre de frames √† ignorer (ou appuyez sur Entr√©e pour 10): ").strip()
    if skip_input.isdigit():
        skip_frames = int(skip_input)
    else:
        skip_frames = 10
    
    # Demander l'affichage de la vid√©o
    print("\n AFFICHAGE VID√âO")
    show_video = input("Voulez-vous voir la vid√©o avec landmarks pendant l'analyse ? (o/n, d√©faut: n): ").strip().lower()
    show_landmarks = show_video.startswith('o') or show_video.startswith('y')
    
    # Demander l'affichage d√©taill√©
    print("\n RAPPORT D√âTAILL√â")
    detailed = input("Voulez-vous un rapport d√©taill√© ? (o/n, d√©faut: o): ").strip().lower()
    detailed_output = not (detailed.startswith('n'))
    
    # Demander l'affichage frame par frame
    print("\n AFFICHAGE FRAME PAR FRAME")
    frame_details = input("Voulez-vous voir les d√©tails de chaque frame en temps r√©el dans la console ? (o/n, d√©faut: n): ").strip().lower()
    show_frame_details = frame_details.startswith('o') or frame_details.startswith('y')
    
    # Afficher la configuration
    print("\n" + "="*50)
    print(" CONFIGURATION DE L'ANALYSE")
    
    print("="*50)
    print(f" Vid√©o : {video_path}")
    
    print(f" Skip frames : {skip_frames}")
    print(f" Affichage vid√©o : {' Oui' if show_landmarks else ' Non'}")
    print(f" Rapport d√©taill√© : {' Oui' if detailed_output else ' Non'}")
    print(f" D√©tails frame par frame : {' Oui' if show_frame_details else ' Non'}")
    
    if show_landmarks:
        print("\n CONTR√îLES VID√âO :")
        print("   - Appuyez sur 'q' pour arr√™ter l'analyse")
        print("   - La fen√™tre s'appellera 'Analyse Vid√©o - Landmarks'")
    
    if show_frame_details:
        print("\n AFFICHAGE CONSOLE :")
        print("   - Chaque frame analys√©e sera affich√©e en d√©tail")
        print("   - Cela peut ralentir l'affichage si vous analysez beaucoup de frames")
    
    input("\nAppuyez sur Entr√©e pour commencer l'analyse...")
    
    print("\n D√©marrage de l'analyse vid√©o...")
    results = analyze_video_file(video_path, skip_frames, detailed_output, show_landmarks, show_frame_details)
    
    if results:
        print("\n R√©sultats de l'analyse :")
        print(f"   Dur√©e vid√©o: {results['metadata']['duration']:.1f}s")
        print(f"   Frames analys√©s: {len(results['face_analysis'])}")
    else:
        print(" √âchec de l'analyse vid√©o")